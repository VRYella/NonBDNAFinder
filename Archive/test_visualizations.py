#!/usr/bin/env python3
"""
Test visualization outputs and exports to ensure all formats work correctly.
"""

import os
import sys
import tempfile
import pandas as pd
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from orchestrator import export_gff3
    from export_utils import export_excel_with_sheets
except ImportError as e:
    print(f"Import warning: {e}")
    print("Some export functions may not be available")


def create_test_dataframe():
    """Create a test DataFrame with motif detection results"""
    data = {
        'S.No': [1, 2, 3, 4, 5],
        'Chromosome/Contig': ['test_seq'] * 5,
        'Class': ['g_quadruplex', 'g_quadruplex', 'triplex', 'i_motif', 'curved_dna'],
        'Subclass': ['canonical', 'relaxed', 'homopurine', 'canonical', 'A_phased'],
        'Start': [100, 500, 200, 300, 400],
        'End': [120, 520, 220, 320, 420],
        'Length': [21, 21, 21, 21, 21],
        'Normalized_Score': [0.95, 0.80, 0.85, 0.75, 0.70],
        'Actual_Score': [0.95, 0.80, 0.85, 0.75, 0.70],
        'GC_Content': [0.8, 0.7, 0.5, 0.9, 0.4],
        'Sequence': ['GGGAGGGTGGGAGGGTGGGAG'] * 5,
        'Motif_ID': ['6.1', '6.2', '5.1', '7.1', '1.1'],
        'Scoring_Method': ['G4Hunter', 'G4Hunter', 'Triplex', 'iMotif', 'Curvature']
    }
    return pd.DataFrame(data)


def test_csv_export():
    """Test CSV export functionality"""
    print("üìä Testing CSV export...")
    
    df = create_test_dataframe()
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
        csv_path = f.name
    
    try:
        # Export to CSV
        df.to_csv(csv_path, index=False)
        
        # Verify the file exists and has content
        assert os.path.exists(csv_path), "CSV file should be created"
        
        # Read back and verify
        df_read = pd.read_csv(csv_path)
        assert len(df_read) == len(df), "CSV should contain all rows"
        assert list(df_read.columns) == list(df.columns), "CSV should have all columns"
        
        print(f"   ‚úÖ CSV export successful: {len(df_read)} rows, {len(df_read.columns)} columns")
        return True
        
    except Exception as e:
        print(f"   ‚ùå CSV export failed: {e}")
        return False
        
    finally:
        if os.path.exists(csv_path):
            os.unlink(csv_path)


def test_excel_export():
    """Test Excel export functionality"""
    print("üìä Testing Excel export...")
    
    df = create_test_dataframe()
    
    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as f:
        excel_path = f.name
    
    try:
        # Export to Excel with multiple sheets
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='All_Motifs', index=False)
            
            # Create separate sheets by class
            for class_name in df['Class'].unique():
                class_df = df[df['Class'] == class_name]
                sheet_name = class_name.replace('_', ' ').title()[:31]  # Excel sheet name limit
                class_df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        # Verify the file exists
        assert os.path.exists(excel_path), "Excel file should be created"
        
        # Read back and verify
        df_read = pd.read_excel(excel_path, sheet_name='All_Motifs')
        assert len(df_read) == len(df), "Excel should contain all rows"
        
        print(f"   ‚úÖ Excel export successful: {len(df_read)} rows with multiple sheets")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Excel export failed: {e}")
        return False
        
    finally:
        if os.path.exists(excel_path):
            os.unlink(excel_path)


def test_parquet_export():
    """Test Parquet export functionality"""
    print("üìä Testing Parquet export...")
    
    df = create_test_dataframe()
    
    with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as f:
        parquet_path = f.name
    
    try:
        # Export to Parquet
        df.to_parquet(parquet_path, index=False)
        
        # Verify the file exists
        assert os.path.exists(parquet_path), "Parquet file should be created"
        
        # Read back and verify
        df_read = pd.read_parquet(parquet_path)
        assert len(df_read) == len(df), "Parquet should contain all rows"
        
        print(f"   ‚úÖ Parquet export successful: {len(df_read)} rows")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Parquet export failed: {e}")
        return False
        
    finally:
        if os.path.exists(parquet_path):
            os.unlink(parquet_path)


def test_gff3_export():
    """Test GFF3 export functionality"""
    print("üìä Testing GFF3 export...")
    
    df = create_test_dataframe()
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.gff3', delete=False) as f:
        gff3_path = f.name
    
    try:
        # Export to GFF3
        try:
            export_gff3(df, gff3_path)
        except NameError:
            # Fallback implementation if export_gff3 not available
            with open(gff3_path, 'w') as f:
                f.write("##gff-version 3\n")
                
                for _, row in df.iterrows():
                    attributes = f"ID=motif_{row['S.No']};Class={row['Class']};Subclass={row['Subclass']};Score={row['Normalized_Score']:.3f};Method={row['Scoring_Method']}"
                    gff_line = f"{row['Chromosome/Contig']}\tNBDFinder\tmotif\t{row['Start']}\t{row['End']}\t{row['Normalized_Score']:.3f}\t.\t.\t{attributes}\n"
                    f.write(gff_line)
        
        # Verify the file exists and has proper header
        assert os.path.exists(gff3_path), "GFF3 file should be created"
        
        with open(gff3_path, 'r') as f:
            content = f.read()
            assert content.startswith("##gff-version 3"), "GFF3 should have proper header"
            lines = content.split('\n')
            data_lines = [line for line in lines if line and not line.startswith('#')]
            assert len(data_lines) == len(df), f"GFF3 should have {len(df)} data lines, got {len(data_lines)}"
        
        print(f"   ‚úÖ GFF3 export successful: {len(data_lines)} entries")
        return True
        
    except Exception as e:
        print(f"   ‚ùå GFF3 export failed: {e}")
        return False
        
    finally:
        if os.path.exists(gff3_path):
            os.unlink(gff3_path)


def test_data_integrity():
    """Test data integrity across different formats"""
    print("üîç Testing data integrity across formats...")
    
    df = create_test_dataframe()
    
    # Test round-trip through different formats
    formats_tested = []
    
    # CSV round-trip
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            csv_path = f.name
        
        df.to_csv(csv_path, index=False)
        df_csv = pd.read_csv(csv_path)
        
        # Check key columns are preserved
        assert df_csv['Class'].tolist() == df['Class'].tolist(), "Class column should be preserved in CSV"
        assert df_csv['Start'].tolist() == df['Start'].tolist(), "Start positions should be preserved in CSV"
        
        formats_tested.append("CSV")
        os.unlink(csv_path)
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è CSV round-trip failed: {e}")
    
    # Parquet round-trip
    try:
        with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as f:
            parquet_path = f.name
        
        df.to_parquet(parquet_path, index=False)
        df_parquet = pd.read_parquet(parquet_path)
        
        # Check data types and values are preserved
        assert df_parquet['Normalized_Score'].dtype.kind == 'f', "Scores should remain float in Parquet"
        assert abs(df_parquet['Normalized_Score'].sum() - df['Normalized_Score'].sum()) < 0.001, "Score totals should match"
        
        formats_tested.append("Parquet")
        os.unlink(parquet_path)
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Parquet round-trip failed: {e}")
    
    print(f"   ‚úÖ Data integrity verified for: {', '.join(formats_tested)}")
    return len(formats_tested) > 0


def main():
    """Run all visualization tests"""
    print("üìä NBDFinder Visualization Export Testing")
    print("=" * 50)
    
    tests = [
        test_csv_export,
        test_excel_export,
        test_parquet_export,
        test_gff3_export,
        test_data_integrity,
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"   ‚ùå Test {test.__name__} failed with exception: {e}")
            results.append(False)
    
    passed = sum(results)
    total = len(results)
    
    print("\n" + "=" * 50)
    if passed == total:
        print("üéâ ALL VISUALIZATION TESTS PASSED!")
        print("=" * 50)
        print("‚úÖ CSV export working")
        print("‚úÖ Excel export working")
        print("‚úÖ Parquet export working")
        print("‚úÖ GFF3 export working")
        print("‚úÖ Data integrity verified")
    else:
        print(f"‚ö†Ô∏è {passed}/{total} TESTS PASSED")
        print("=" * 50)
        
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)