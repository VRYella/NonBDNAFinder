{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lw-overview",
   "metadata": {},
   "source": [
    "# NonBDNA Finder — Lightweight Comparative Notebook\n\n## Purpose\nHigh-performance detection of Non-B DNA structural motifs with **lightweight, comparative-only output**.\nAll heavy per-file visualisations (linear tracks, KDE, violin, pie, network, UpSet) are **skipped**.\n\n## What this notebook produces\n| Output | Description |\n|--------|-------------|\n| **Per-file summary table** | Sequences, bp, GC%, motifs, density/kb, coverage% |\n| **Class statistics table** | Count, mean length/score, density/kb, coverage% |\n| **Subclass statistics table** | Count, mean length/score, density/kb, coverage% (top 30) |\n| **Cross-file class distribution** | Bar chart: motif counts per class across all files |\n| **Cross-file density bar chart** | Motifs/kb comparison across files |\n| **Cross-file coverage bar chart** | Coverage% comparison across files |\n| **Class density heatmap** | Files × Classes (motifs/kb) |\n| **Class coverage heatmap** | Files × Classes (coverage%) |\n| **Hybrid & Cluster comparison** | Counts across files |\n| **GFF feature × class heatmap** | When GFF annotation files are present |\n| **Global comprehensive stats** | 25 structural metrics table |\n\n## Performance strategy\n- **ProcessPoolExecutor** for all sequence-level parallelism (bypasses Python GIL)\n- **All available CPU cores** utilised adaptively (`os.cpu_count()`)\n- **Parquet streaming** keeps RAM low for large genomes\n- **Vectorised NumPy/Pandas** operations throughout\n- **Lazy GC** (`gc.collect()`) after each file to free memory promptly\n\n## How to run\n1. Edit `FASTA_INPUT` and `OUTPUT_DIR` in **Cell 1**\n2. Run **Cell 1** (setup)\n3. Run **Cell 2** (detection + comparative outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "lw-setup",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 1 * SETUP -- imports, configuration, helpers\n",
    "# Edit FASTA_INPUT and OUTPUT_DIR, then run this cell before Cell 2.\n",
    "# =============================================================================\n",
    "\n",
    "import sys, os, importlib, glob, gc, time, datetime, re, warnings\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "_REPO_ROOT = os.path.abspath(os.getcwd())\n",
    "if _REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, _REPO_ROOT)\n",
    "\n",
    "# -- Auto-install missing packages -------------------------------------------\n",
    "_REQUIRED = [\n",
    "    ('psutil',    'psutil>=5.8'),\n",
    "    ('pandas',    'pandas>=1.3'),\n",
    "    ('numpy',     'numpy>=1.21'),\n",
    "    ('matplotlib','matplotlib>=3.5'),\n",
    "    ('seaborn',   'seaborn>=0.11'),\n",
    "    ('openpyxl',  'openpyxl>=3.0'),\n",
    "    ('tqdm',      'tqdm>=4.64'),\n",
    "    ('pyarrow',   'pyarrow>=10.0'),\n",
    "]\n",
    "_miss = [p for m, p in _REQUIRED if importlib.util.find_spec(m) is None]\n",
    "if _miss:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *_miss, '-q'])\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML, Image\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# -- Optional fast FASTA parser -----------------------------------------------\n",
    "try:\n",
    "    import pyfastx as _pyfastx\n",
    "    _HAS_PYFASTX = True\n",
    "except ImportError:\n",
    "    _HAS_PYFASTX = False\n",
    "    try:\n",
    "        from Bio import SeqIO as _SeqIO\n",
    "        _HAS_SEQIO = True\n",
    "    except ImportError:\n",
    "        _HAS_SEQIO = False\n",
    "\n",
    "# -- USER CONFIGURATION -------------------------------------------------------\n",
    "FASTA_INPUT        = ['*.fna', '*.fasta']  # path, wildcard, or list\n",
    "OUTPUT_DIR         = 'notebook_reports_lw' # output directory\n",
    "ENABLED_CLASSES    = None                  # None = all; e.g. ['G-Quadruplex','Z-DNA']\n",
    "RAM_OVERRIDE_BYTES = None                  # None = auto\n",
    "\n",
    "# -- Large-chromosome chunking ------------------------------------------------\n",
    "LARGE_CHR_THRESHOLD_MB  = 50  # split chromosomes larger than this (Mb)\n",
    "GENOME_CHUNK_SIZE_MB    = 2   # sub-chunk size (Mb)\n",
    "GENOME_CHUNK_OVERLAP_KB = 5   # overlap between sub-chunks (kb)\n",
    "\n",
    "# -- GPU detection ------------------------------------------------------------\n",
    "def _detect_gpu():\n",
    "    for lib, _attr in [('torch', 'cuda'), ('cupy', None)]:\n",
    "        try:\n",
    "            m = importlib.import_module(lib)\n",
    "            if lib == 'torch' and m.cuda.is_available():\n",
    "                return 'cuda', m.cuda.get_device_name(0)\n",
    "            elif lib == 'cupy':\n",
    "                m.array([1]); return 'cupy', 'CUDA GPU'\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None, None\n",
    "\n",
    "GPU_BACKEND, GPU_NAME = _detect_gpu()\n",
    "print(f'\\u2705 Deps OK | Python {sys.version.split()[0]} | '\n",
    "      f'GPU: {GPU_BACKEND + \"(\" + GPU_NAME + \")\" if GPU_BACKEND else \"none (CPU)\"}')\n",
    "\n",
    "# -- Resolve FASTA files ------------------------------------------------------\n",
    "def _resolve(inp):\n",
    "    out = []\n",
    "    for p in ([inp] if isinstance(inp, str) else list(inp)):\n",
    "        hits = glob.glob(p); out.extend(hits)\n",
    "        if not hits and os.path.isfile(p): out.append(p)\n",
    "    return sorted({str(Path(f).resolve()) for f in out})\n",
    "\n",
    "def _seq_lengths(p):\n",
    "    L, c = [], 0\n",
    "    with open(p) as fh:\n",
    "        for ln in fh:\n",
    "            s = ln.strip()\n",
    "            if s.startswith('>'):\n",
    "                if c: L.append(c); c = 0\n",
    "            else:\n",
    "                c += len(s)\n",
    "    if c: L.append(c)\n",
    "    return L\n",
    "\n",
    "def _stream_fasta(fasta_path):\n",
    "    \"\"\"Stream (name, seq) pairs without loading the whole genome.\"\"\"\n",
    "    if _HAS_PYFASTX:\n",
    "        for seq in _pyfastx.Fasta(str(fasta_path), build_index=False):\n",
    "            yield seq.name, seq.seq\n",
    "    elif _HAS_SEQIO:\n",
    "        with open(fasta_path) as fh:\n",
    "            for rec in _SeqIO.parse(fh, 'fasta'):\n",
    "                yield rec.id, str(rec.seq)\n",
    "    else:\n",
    "        name, parts = None, []\n",
    "        with open(fasta_path) as fh:\n",
    "            for ln in fh:\n",
    "                s = ln.rstrip('\\n')\n",
    "                if s.startswith('>'):\n",
    "                    if name is not None:\n",
    "                        yield name, ''.join(parts)\n",
    "                    name = s[1:].split()[0]; parts = []\n",
    "                else:\n",
    "                    parts.append(s)\n",
    "        if name is not None:\n",
    "            yield name, ''.join(parts)\n",
    "\n",
    "FASTA_FILES = _resolve(FASTA_INPUT)\n",
    "if not FASTA_FILES:\n",
    "    raise FileNotFoundError(f'No FASTA files found for: {FASTA_INPUT}')\n",
    "\n",
    "FILE_TYPES = {}\n",
    "for fp in FASTA_FILES:\n",
    "    ls = _seq_lengths(fp)\n",
    "    FILE_TYPES[fp] = ('single' if len(ls) == 1 else\n",
    "                      'multi_equal' if len(set(ls)) == 1 else 'multi')\n",
    "\n",
    "GFF_MAP = {}\n",
    "for fp in FASTA_FILES:\n",
    "    stem, parent = Path(fp).stem, Path(fp).parent\n",
    "    for ext in ('.gff3', '.gff'):\n",
    "        cand = parent / (stem + ext)\n",
    "        if cand.exists(): GFF_MAP[fp] = str(cand); break\n",
    "\n",
    "print(f'\\n\\U0001f4c2 Input files: {len(FASTA_FILES)}')\n",
    "for fp in FASTA_FILES:\n",
    "    gff_tag = f'  +GFF: {Path(GFF_MAP[fp]).name}' if fp in GFF_MAP else ''\n",
    "    print(f'   [{FILE_TYPES[fp]:12s}]  {Path(fp).name}{gff_tag}')\n",
    "\n",
    "# -- Adaptive resource planning -----------------------------------------------\n",
    "from Utilities.system_resource_inspector import SystemResourceInspector\n",
    "from Utilities.adaptive_chunk_planner    import AdaptiveChunkPlanner\n",
    "from Utilities.nonbscanner               import analyze_sequence as _nbf_analyze\n",
    "from Utilities.utilities                 import (\n",
    "    read_fasta_file,\n",
    "    compute_comprehensive_genome_stats,\n",
    ")\n",
    "\n",
    "_insp   = SystemResourceInspector()\n",
    "_budget = RAM_OVERRIDE_BYTES or _insp.get_memory_budget()\n",
    "_cpus   = _insp.get_cpu_count()\n",
    "_total  = max(sum(os.path.getsize(f) for f in FASTA_FILES if os.path.exists(f)), 1_000)\n",
    "_plan   = AdaptiveChunkPlanner().plan(_total, _budget, _cpus)\n",
    "CHUNK_SIZE, CHUNK_OVERLAP = _plan['chunk_size'], _plan['overlap']\n",
    "\n",
    "# Use all available cores; fallback of 2 keeps things safe on minimal hardware.\n",
    "N_WORKERS = max(1, (os.cpu_count() or 2) - 1)\n",
    "EXEC_MODE = _plan['mode']\n",
    "\n",
    "# Derived chunking parameters (Mb / kb -> bp)\n",
    "_LARGE_CHR_THRESHOLD  = LARGE_CHR_THRESHOLD_MB  * 1_000_000\n",
    "_GENOME_CHUNK_SIZE    = GENOME_CHUNK_SIZE_MB     * 1_000_000\n",
    "_GENOME_CHUNK_OVERLAP = GENOME_CHUNK_OVERLAP_KB  * 1_000\n",
    "\n",
    "# Timestamp is UTC; output directory names include 'Z' suffix to reflect this.\n",
    "_RUN_TS = datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "_BASE   = Path(OUTPUT_DIR) / _RUN_TS\n",
    "_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'\\u2699\\ufe0f  RAM {_budget/1e9:.2f} GB | '\n",
    "      f'chunk={CHUNK_SIZE:,} overlap={CHUNK_OVERLAP:,} '\n",
    "      f'workers={N_WORKERS} mode={EXEC_MODE}')\n",
    "_fasta_backend = 'pyfastx' if _HAS_PYFASTX else ('SeqIO' if _HAS_SEQIO else 'built-in')\n",
    "print(f'   FASTA stream: {_fasta_backend} | '\n",
    "      f'large-chr chunk: {GENOME_CHUNK_SIZE_MB} Mb + {GENOME_CHUNK_OVERLAP_KB} kb overlap')\n",
    "print(f'\\U0001f4c2 Run output: {_BASE}')\n",
    "\n",
    "# -- Shared helpers -----------------------------------------------------------\n",
    "def _savefig(fig, path):\n",
    "    \"\"\"Save figure; display inline as a small PNG (lightweight output).\"\"\"\n",
    "    fig.savefig(str(path), dpi=120, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    display(Image(str(path)))\n",
    "\n",
    "def _safe_fname(s):\n",
    "    return re.sub(r'[^\\w\\-]', '_', str(s))\n",
    "\n",
    "def _gc_and_length(fasta_path):\n",
    "    gc = total = 0\n",
    "    with open(fasta_path) as fh:\n",
    "        for ln in fh:\n",
    "            s = ln.strip()\n",
    "            if not s or s.startswith('>'): continue\n",
    "            su = s.upper(); gc += su.count('G') + su.count('C'); total += len(su)\n",
    "    return (round(gc / total * 100, 2) if total else 0.0), total\n",
    "\n",
    "def _merge_coverage(intervals, cap=None):\n",
    "    \"\"\"Vectorised merge of intervals; returns total covered bases.\"\"\"\n",
    "    if len(intervals) == 0: return 0\n",
    "    intervals = np.array(intervals, dtype=np.int64)\n",
    "    if cap is not None:\n",
    "        intervals[:, 1] = np.minimum(intervals[:, 1], cap)\n",
    "    intervals = intervals[intervals[:, 1] > intervals[:, 0]]\n",
    "    if len(intervals) == 0: return 0\n",
    "    intervals = intervals[np.argsort(intervals[:, 0])]\n",
    "    s, e = intervals[0]; covered = 0\n",
    "    for cs, ce in intervals[1:]:\n",
    "        if cs <= e:\n",
    "            e = max(e, ce)\n",
    "        else:\n",
    "            covered += e - s; s, e = cs, ce\n",
    "    return covered + e - s\n",
    "\n",
    "def _coverage(df, seq_lengths_dict):\n",
    "    if df.empty or not seq_lengths_dict: return 0.0\n",
    "    total_len = sum(seq_lengths_dict.values())\n",
    "    if total_len == 0: return 0.0\n",
    "    covered = sum(\n",
    "        _merge_coverage(grp[['Start', 'End']].values, cap=seq_lengths_dict.get(sn, 0))\n",
    "        for sn, grp in df.groupby('Sequence_Name')\n",
    "    )\n",
    "    return round(covered / total_len * 100, 2)\n",
    "\n",
    "def _class_density_coverage(df, all_results):\n",
    "    total_bp = sum(sum(r['seq_lengths'].values()) for r in all_results.values())\n",
    "    if total_bp == 0 or df.empty: return {}\n",
    "    out = {}\n",
    "    for cls, grp in df.groupby('Class'):\n",
    "        cov_bp = 0\n",
    "        for stem, res in all_results.items():\n",
    "            sub = grp[grp['Source_File'] == Path(res['path']).name]\n",
    "            if sub.empty: continue\n",
    "            for sn, sg in sub.groupby('Sequence_Name'):\n",
    "                sq_len = res['seq_lengths'].get(sn, 0)\n",
    "                cov_bp += _merge_coverage(sg[['Start', 'End']].values, cap=sq_len)\n",
    "        out[cls] = {'Density_per_kb': round(len(grp) / total_bp * 1000, 4),\n",
    "                    'Coverage_pct':   round(cov_bp / total_bp * 100, 3)}\n",
    "    return out\n",
    "\n",
    "def _subclass_density_coverage(df, all_results):\n",
    "    total_bp = sum(sum(r['seq_lengths'].values()) for r in all_results.values())\n",
    "    if total_bp == 0 or df.empty: return {}\n",
    "    out = {}\n",
    "    for sc, grp in df.groupby('Subclass'):\n",
    "        cov_bp = 0\n",
    "        for stem, res in all_results.items():\n",
    "            sub = grp[grp['Source_File'] == Path(res['path']).name]\n",
    "            if sub.empty: continue\n",
    "            for sn, sg in sub.groupby('Sequence_Name'):\n",
    "                sq_len = res['seq_lengths'].get(sn, 0)\n",
    "                cov_bp += _merge_coverage(sg[['Start', 'End']].values, cap=sq_len)\n",
    "        out[sc] = {'Density_per_kb': round(len(grp) / total_bp * 1000, 4),\n",
    "                   'Coverage_pct':   round(cov_bp / total_bp * 100, 3)}\n",
    "    return out\n",
    "\n",
    "def _parse_gff(gff_path):\n",
    "    rows = []\n",
    "    with open(gff_path) as fh:\n",
    "        for ln in fh:\n",
    "            if ln.startswith('#'): continue\n",
    "            p = ln.strip().split('\\t')\n",
    "            if len(p) < 9: continue\n",
    "            rows.append({'Seq': p[0], 'GFF_Type': p[2],\n",
    "                         'GFF_Start': int(p[3]), 'GFF_End': int(p[4])})\n",
    "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "lw-analysis",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 2 * ANALYSIS -- parallel detection + comparative outputs only\n",
    "# Run Cell 1 first.\n",
    "# =============================================================================\n",
    "\n",
    "_WALL_START = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    import psutil as _psutil\n",
    "    _proc = _psutil.Process()\n",
    "    def _mem_mb(): return _proc.memory_info().rss / 1e6\n",
    "except ImportError:\n",
    "    def _mem_mb(): return float('nan')\n",
    "\n",
    "# -- Detection: ProcessPoolExecutor for maximum parallelism -------------------\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from Utilities.genome_worker import process_chromosome\n",
    "\n",
    "RESULTS_BY_FILE = {}\n",
    "GFF_RESULTS     = {}\n",
    "\n",
    "for fasta_path in tqdm(FASTA_FILES, desc='Files', unit='file'):\n",
    "    stem  = Path(fasta_path).stem\n",
    "    ftype = FILE_TYPES[fasta_path]\n",
    "    fdir  = _BASE / stem\n",
    "    fdir.mkdir(parents=True, exist_ok=True)\n",
    "    _parquet_dir = str(fdir / '_parquet')\n",
    "    Path(_parquet_dir).mkdir(exist_ok=True)\n",
    "    tqdm.write(f'\\n\\u2500\\u2500 {stem}  [{ftype}] \\u2500\\u2500')\n",
    "\n",
    "    _seq_items = list(_stream_fasta(fasta_path))\n",
    "    sl_map = {sn: len(sq) for sn, sq in _seq_items}\n",
    "\n",
    "    _worker_args = [\n",
    "        (\n",
    "            sn, sq,\n",
    "            Path(fasta_path).name, ftype,\n",
    "            _LARGE_CHR_THRESHOLD,\n",
    "            _GENOME_CHUNK_SIZE,\n",
    "            _GENOME_CHUNK_OVERLAP,\n",
    "            ENABLED_CLASSES,\n",
    "            _parquet_dir,\n",
    "            CHUNK_SIZE, CHUNK_OVERLAP,\n",
    "        )\n",
    "        for sn, sq in _seq_items\n",
    "    ]\n",
    "    del _seq_items; gc.collect()\n",
    "\n",
    "    _t0 = time.perf_counter()\n",
    "    _parquet_paths = []\n",
    "    _total_motifs  = 0\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as _pool:\n",
    "        # NOTE: sequence strings are pickled per worker — for very large chromosomes\n",
    "        # (>100 Mb) this increases peak RSS. The genome_worker writes results to\n",
    "        # Parquet on disk, so the parent process only holds metadata in memory.\n",
    "        _futs = {_pool.submit(process_chromosome, a): a[0] for a in _worker_args}\n",
    "        for _fut in tqdm(as_completed(_futs), total=len(_futs),\n",
    "                         desc=f'  seqs({stem})', leave=False):\n",
    "            _sn = _futs[_fut]\n",
    "            try:\n",
    "                _sn_r, _ppath, _n, _t = _fut.result()\n",
    "                if _ppath:\n",
    "                    _parquet_paths.append(_ppath)\n",
    "                _total_motifs += _n\n",
    "                tqdm.write(f'  \\u25b8 {_sn_r[:55]}  \\u2192 {_n:,} motifs ({_t:.1f}s)')\n",
    "            except Exception as _e:\n",
    "                tqdm.write(f'  \\u26a0\\ufe0f  {_sn[:55]}  \\u2192 skipped ({_e})')\n",
    "\n",
    "    _elapsed = time.perf_counter() - _t0\n",
    "    tqdm.write(f'  \\u2705 {_total_motifs:,} motifs in {_elapsed:.1f}s | RAM {_mem_mb():.0f} MB')\n",
    "\n",
    "    if _parquet_paths:\n",
    "        import pyarrow.parquet as _pq_local\n",
    "        df = pd.concat([pd.read_parquet(p) for p in _parquet_paths], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    for _col, _dflt in [('Class', 'Unknown'), ('Subclass', 'Other'),\n",
    "                         ('Start', 0), ('End', 0), ('Length', 0),\n",
    "                         ('Score', 0.0), ('Strand', '+'), ('Sequence_Name', '')]:\n",
    "        if _col not in df.columns: df[_col] = _dflt\n",
    "    if not df.empty:\n",
    "        df['Length'] = np.where(df['Length'] == 0,\n",
    "                                (df['End'] - df['Start']).clip(lower=0), df['Length'])\n",
    "    df['Source_File'] = Path(fasta_path).name\n",
    "    df['File_Type']   = ftype\n",
    "\n",
    "    if not df.empty:\n",
    "        df.to_parquet(str(fdir / 'motifs.parquet'), index=False)\n",
    "        df.to_csv(str(fdir / 'motifs.csv'), encoding='utf-8-sig', index=False)\n",
    "\n",
    "    # GFF region tagging (if available)\n",
    "    if fasta_path in GFF_MAP:\n",
    "        _gff_raw = _parse_gff(GFF_MAP[fasta_path])\n",
    "        if not _gff_raw.empty and not df.empty:\n",
    "            _gff_tagged = df.merge(\n",
    "                _gff_raw, left_on='Sequence_Name', right_on='Seq', how='inner')\n",
    "            _gff_tagged = _gff_tagged[\n",
    "                (_gff_tagged['Start'] >= _gff_tagged['GFF_Start']) &\n",
    "                (_gff_tagged['End']   <= _gff_tagged['GFF_End'])\n",
    "            ]\n",
    "            GFF_RESULTS[stem] = {'df': _gff_tagged, 'path': GFF_MAP[fasta_path]}\n",
    "\n",
    "    RESULTS_BY_FILE[stem] = {\n",
    "        'df': df, 'folder': fdir, 'file_type': ftype,\n",
    "        'path': fasta_path, 'seq_lengths': sl_map,\n",
    "    }\n",
    "    gc.collect()\n",
    "\n",
    "_wall_detect = time.perf_counter() - _WALL_START\n",
    "print(f'\\n\\u2705 Detection complete -- {len(RESULTS_BY_FILE)} file(s) in {_wall_detect:.1f}s | '\n",
    "      f'RAM {_mem_mb():.0f} MB')\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICS & COMPARATIVE TABLES\n",
    "# =============================================================================\n",
    "\n",
    "_dfs        = [r['df'] for r in RESULTS_BY_FILE.values() if not r['df'].empty]\n",
    "_master_df  = pd.concat(_dfs, ignore_index=True) if _dfs else pd.DataFrame()\n",
    "_master_dir = _BASE / '_master'; _master_dir.mkdir(exist_ok=True)\n",
    "\n",
    "_gff_dfs = [v['df'] for v in GFF_RESULTS.values() if not v['df'].empty]\n",
    "_gff_df  = pd.concat(_gff_dfs, ignore_index=True) if _gff_dfs else pd.DataFrame()\n",
    "\n",
    "_tables = {}\n",
    "\n",
    "if not _master_df.empty:\n",
    "    # Table 1: Per-file summary\n",
    "    _pf_rows = []\n",
    "    for stem, res in RESULTS_BY_FILE.items():\n",
    "        df_r = res['df']\n",
    "        gc_pct, seq_len = _gc_and_length(res['path'])\n",
    "        n = len(df_r)\n",
    "        _pf_rows.append({\n",
    "            'File':           Path(res['path']).name,\n",
    "            'File_Type':      res['file_type'],\n",
    "            'Sequences':      len(res['seq_lengths']),\n",
    "            'Total_bp':       seq_len,\n",
    "            'GC_Percent':     gc_pct,\n",
    "            'Total_Motifs':   n,\n",
    "            'Classes':        df_r['Class'].nunique()    if not df_r.empty else 0,\n",
    "            'Subclasses':     df_r['Subclass'].nunique() if not df_r.empty else 0,\n",
    "            'Hybrids':        int((df_r['Class'] == 'Hybrid').sum())             if not df_r.empty else 0,\n",
    "            'Clusters':       int((df_r['Class'] == 'Non-B_DNA_Clusters').sum()) if not df_r.empty else 0,\n",
    "            'Density_per_kb': round(n / seq_len * 1000, 4) if seq_len else 0.0,\n",
    "            'Coverage_pct':   _coverage(df_r, res['seq_lengths']),\n",
    "        })\n",
    "    _tables['1_per_file_summary'] = pd.DataFrame(_pf_rows)\n",
    "\n",
    "    # Table 2: Class statistics\n",
    "    _cls_dc = _class_density_coverage(_master_df, RESULTS_BY_FILE)\n",
    "    _tables['2_class_statistics'] = (\n",
    "        _master_df.groupby('Class')\n",
    "        .agg(Total_Count=('Class', 'count'),\n",
    "             Mean_Length=('Length', 'mean'),\n",
    "             Mean_Score=('Score', 'mean'))\n",
    "        .round(3).reset_index()\n",
    "        .assign(\n",
    "            Density_per_kb=lambda d: d['Class'].map(\n",
    "                lambda c: _cls_dc.get(c, {}).get('Density_per_kb', 0)),\n",
    "            Coverage_pct=lambda d: d['Class'].map(\n",
    "                lambda c: _cls_dc.get(c, {}).get('Coverage_pct', 0)),\n",
    "        )\n",
    "        .sort_values('Total_Count', ascending=False)\n",
    "        [['Class', 'Total_Count', 'Mean_Length', 'Mean_Score', 'Density_per_kb', 'Coverage_pct']]\n",
    "    )\n",
    "\n",
    "    # Table 3: Subclass statistics (top 30 by count)\n",
    "    _sc_dc = _subclass_density_coverage(_master_df, RESULTS_BY_FILE)\n",
    "    _tables['3_subclass_statistics'] = (\n",
    "        _master_df.groupby('Subclass')\n",
    "        .agg(Total_Count=('Subclass', 'count'),\n",
    "             Mean_Length=('Length', 'mean'),\n",
    "             Mean_Score=('Score', 'mean'))\n",
    "        .round(3).reset_index()\n",
    "        .assign(\n",
    "            Density_per_kb=lambda d: d['Subclass'].map(\n",
    "                lambda s: _sc_dc.get(s, {}).get('Density_per_kb', 0)),\n",
    "            Coverage_pct=lambda d: d['Subclass'].map(\n",
    "                lambda s: _sc_dc.get(s, {}).get('Coverage_pct', 0)),\n",
    "        )\n",
    "        .sort_values('Total_Count', ascending=False)\n",
    "        .head(30)\n",
    "        [['Subclass', 'Total_Count', 'Mean_Length', 'Mean_Score', 'Density_per_kb', 'Coverage_pct']]\n",
    "    )\n",
    "\n",
    "    # Table 4: File x Class density pivot\n",
    "    _dens_rows = []\n",
    "    for (fname, cls), grp in _master_df.groupby(['Source_File', 'Class']):\n",
    "        stem_k = Path(fname).stem\n",
    "        res    = RESULTS_BY_FILE.get(stem_k, {})\n",
    "        slen   = max(sum(res.get('seq_lengths', {1: 1}).values()), 1)\n",
    "        _dens_rows.append({'Source_File': fname, 'Class': cls,\n",
    "                            'Density_per_kb': round(len(grp) / slen * 1000, 4)})\n",
    "    _tables['4_class_density_pivot'] = (\n",
    "        pd.DataFrame(_dens_rows)\n",
    "        .pivot_table(index='Source_File', columns='Class',\n",
    "                     values='Density_per_kb', fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Table 5: File x Class coverage pivot\n",
    "    _cov_rows = []\n",
    "    for fname in _master_df['Source_File'].unique():\n",
    "        stem_k = Path(fname).stem\n",
    "        res    = RESULTS_BY_FILE.get(stem_k)\n",
    "        if not res: continue\n",
    "        sub_file = _master_df[_master_df['Source_File'] == fname]\n",
    "        slen     = max(sum(res['seq_lengths'].values()), 1)\n",
    "        for cls, grp in sub_file.groupby('Class'):\n",
    "            cov = sum(\n",
    "                _merge_coverage(sg[['Start', 'End']].values,\n",
    "                                cap=res['seq_lengths'].get(sn, 0))\n",
    "                for sn, sg in grp.groupby('Sequence_Name')\n",
    "            )\n",
    "            _cov_rows.append({'Source_File': fname, 'Class': cls,\n",
    "                               'Coverage_pct': round(cov / slen * 100, 3)})\n",
    "    _tables['5_class_coverage_pivot'] = (\n",
    "        pd.DataFrame(_cov_rows)\n",
    "        .pivot_table(index='Source_File', columns='Class',\n",
    "                     values='Coverage_pct', fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "if not _gff_df.empty:\n",
    "    _tables['6_gff_motifs_per_feature'] = (\n",
    "        _gff_df.groupby(['GFF_Type', 'Class']).size().reset_index(name='Count')\n",
    "        .sort_values('Count', ascending=False)\n",
    "    )\n",
    "\n",
    "# -- Export tables to CSV -----------------------------------------------------\n",
    "if not _master_df.empty:\n",
    "    _master_df.to_parquet(str(_master_dir / 'master_motifs.parquet'), index=False)\n",
    "    _master_df.to_csv(str(_master_dir / 'master_motifs.csv'), encoding='utf-8-sig', index=False)\n",
    "if not _gff_df.empty:\n",
    "    _gff_df.to_csv(str(_master_dir / 'gff_region_motifs_all.csv'),\n",
    "                   encoding='utf-8-sig', index=False)\n",
    "for tname, tdf in _tables.items():\n",
    "    tdf.to_csv(str(_master_dir / f'{tname}.csv'), encoding='utf-8-sig', index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARATIVE VISUALISATIONS (final summary charts only)\n",
    "# =============================================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('COMPARATIVE VISUALISATIONS')\n",
    "print('=' * 70)\n",
    "\n",
    "if not _master_df.empty:\n",
    "    _pf_summary = _tables.get('1_per_file_summary', pd.DataFrame())\n",
    "    _n_files    = len(RESULTS_BY_FILE)\n",
    "\n",
    "    # 1. Global class distribution (horizontal bar)\n",
    "    cc = _master_df['Class'].value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, len(cc) * 0.45)))\n",
    "    ax.barh(cc.index[::-1], cc.values[::-1], color='steelblue')\n",
    "    ax.set_xlabel('Motif Count')\n",
    "    ax.set_title('Global Class Distribution')\n",
    "    for i, v in enumerate(cc.values[::-1]):\n",
    "        ax.text(v + 0.3, i, str(v), va='center', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    _savefig(fig, _master_dir / 'comparative_class_distribution.png')\n",
    "\n",
    "    # 2. Global class density (motifs/kb)\n",
    "    cls_stat = _tables['2_class_statistics'].set_index('Class')['Density_per_kb']\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, len(cls_stat) * 0.45)))\n",
    "    ax.barh(cls_stat.index[::-1], cls_stat.values[::-1], color='teal')\n",
    "    ax.set_xlabel('Motifs per kb')\n",
    "    ax.set_title('Global Class Density (motifs/kb)')\n",
    "    for i, v in enumerate(cls_stat.values[::-1]):\n",
    "        ax.text(v, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    _savefig(fig, _master_dir / 'comparative_class_density.png')\n",
    "\n",
    "    # 3. Global class coverage (%)\n",
    "    cls_cov_s = _tables['2_class_statistics'].set_index('Class')['Coverage_pct']\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, len(cls_cov_s) * 0.45)))\n",
    "    ax.barh(cls_cov_s.index[::-1], cls_cov_s.values[::-1], color='mediumseagreen')\n",
    "    ax.set_xlabel('Coverage (%)')\n",
    "    ax.set_title('Global Class Coverage (%)')\n",
    "    for i, v in enumerate(cls_cov_s.values[::-1]):\n",
    "        ax.text(v, i, f'{v:.3f}%', va='center', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    _savefig(fig, _master_dir / 'comparative_class_coverage.png')\n",
    "\n",
    "    # 4. Top-20 subclass distribution\n",
    "    sc_all = _master_df['Subclass'].value_counts().head(20)\n",
    "    fig, ax = plt.subplots(figsize=(8, max(4, len(sc_all) * 0.4)))\n",
    "    ax.barh(sc_all.index[::-1], sc_all.values[::-1], color='darkorange')\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_title('Global Subclass Distribution (top 20)')\n",
    "    plt.tight_layout()\n",
    "    _savefig(fig, _master_dir / 'comparative_subclass_distribution.png')\n",
    "\n",
    "    # 5. File-level density & coverage comparison\n",
    "    if not _pf_summary.empty:\n",
    "        for col, label, color, title, fn in [\n",
    "            ('Density_per_kb', 'Motifs per kb', 'steelblue',\n",
    "             'Motif Density Across Files', 'comparative_density_across_files.png'),\n",
    "            ('Coverage_pct', 'Coverage (%)', 'mediumseagreen',\n",
    "             'Non-B DNA Coverage Across Files', 'comparative_coverage_across_files.png'),\n",
    "        ]:\n",
    "            _pfs = _pf_summary.sort_values(col, ascending=False)\n",
    "            fig, ax = plt.subplots(figsize=(max(6, len(_pfs) * 1.4), 4))\n",
    "            bars = ax.bar(\n",
    "                _pfs['File'].apply(lambda x: Path(x).stem[:25]),\n",
    "                _pfs[col], color=color,\n",
    "            )\n",
    "            ax.bar_label(bars,\n",
    "                         fmt='%.3f' if col == 'Density_per_kb' else '%.1f%%',\n",
    "                         padding=2, fontsize=8)\n",
    "            ax.set_ylabel(label); ax.set_title(title)\n",
    "            if col == 'Coverage_pct': ax.set_ylim(0, 100)\n",
    "            plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "            _savefig(fig, _master_dir / fn)\n",
    "\n",
    "    # 6. Class density heatmap (files x classes)\n",
    "    if '4_class_density_pivot' in _tables and not _tables['4_class_density_pivot'].empty:\n",
    "        _dp = _tables['4_class_density_pivot'].set_index('Source_File')\n",
    "        if not _dp.empty:\n",
    "            fig, ax = plt.subplots(figsize=(max(10, len(_dp.columns) * 1.2),\n",
    "                                            max(4,  len(_dp) * 0.7)))\n",
    "            sns.heatmap(_dp, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax,\n",
    "                        linewidths=0.4, cbar_kws={'label': 'Motifs per kb'})\n",
    "            ax.set_title('Class Density Heatmap (motifs/kb) -- Files x Classes')\n",
    "            ax.set_xlabel('Non-B Class'); ax.set_ylabel('File')\n",
    "            plt.tight_layout()\n",
    "            _savefig(fig, _master_dir / 'comparative_class_density_heatmap.png')\n",
    "\n",
    "    # 7. Class coverage heatmap (files x classes)\n",
    "    if '5_class_coverage_pivot' in _tables and not _tables['5_class_coverage_pivot'].empty:\n",
    "        _cp = _tables['5_class_coverage_pivot'].set_index('Source_File')\n",
    "        if not _cp.empty:\n",
    "            fig, ax = plt.subplots(figsize=(max(10, len(_cp.columns) * 1.2),\n",
    "                                            max(4,  len(_cp) * 0.7)))\n",
    "            sns.heatmap(_cp, annot=True, fmt='.3f', cmap='Blues', ax=ax,\n",
    "                        linewidths=0.4, cbar_kws={'label': 'Coverage %'})\n",
    "            ax.set_title('Class Coverage Heatmap (%) -- Files x Classes')\n",
    "            ax.set_xlabel('Non-B Class'); ax.set_ylabel('File')\n",
    "            plt.tight_layout()\n",
    "            _savefig(fig, _master_dir / 'comparative_class_coverage_heatmap.png')\n",
    "\n",
    "    # 8. Multi-file class count comparison (grouped bar; only when >= 2 files)\n",
    "    if _n_files >= 2:\n",
    "        _cls_pivot = _master_df.groupby(['Source_File', 'Class']).size().unstack(fill_value=0)\n",
    "        fig, ax = plt.subplots(figsize=(max(10, len(_cls_pivot) * 1.4),\n",
    "                                        max(4, len(_cls_pivot.columns) * 0.5)))\n",
    "        _cls_pivot.plot(kind='bar', ax=ax, colormap='tab20', width=0.8)\n",
    "        ax.set_xlabel('File'); ax.set_ylabel('Motif Count')\n",
    "        ax.set_title('Class Distribution -- All Files Comparison')\n",
    "        ax.legend(title='Class', bbox_to_anchor=(1, 1), fontsize=8)\n",
    "        plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "        _savefig(fig, _master_dir / 'comparative_all_files_class_comparison.png')\n",
    "\n",
    "        # Top-20 subclass comparison across files\n",
    "        _top_subs  = _master_df['Subclass'].value_counts().head(20).index\n",
    "        _sub_pivot = _master_df.groupby(['Source_File', 'Subclass']).size().unstack(fill_value=0)\n",
    "        _sub_pivot = _sub_pivot[[c for c in _top_subs if c in _sub_pivot.columns]]\n",
    "        if not _sub_pivot.empty:\n",
    "            fig, ax = plt.subplots(figsize=(max(10, len(_sub_pivot) * 1.4),\n",
    "                                            max(4, len(_sub_pivot.columns) * 0.4)))\n",
    "            _sub_pivot.plot(kind='bar', ax=ax, colormap='tab20', width=0.8)\n",
    "            ax.set_xlabel('File'); ax.set_ylabel('Motif Count')\n",
    "            ax.set_title('Subclass Distribution -- All Files (top 20)')\n",
    "            ax.legend(title='Subclass', bbox_to_anchor=(1, 1), fontsize=7)\n",
    "            plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "            _savefig(fig, _master_dir / 'comparative_all_files_subclass_comparison.png')\n",
    "\n",
    "    # 9. Hybrid & Cluster comparison\n",
    "    if not _pf_summary.empty and (\n",
    "        _pf_summary['Hybrids'].sum() > 0 or _pf_summary['Clusters'].sum() > 0\n",
    "    ):\n",
    "        x = np.arange(len(_pf_summary)); w = 0.35\n",
    "        labels = _pf_summary['File'].apply(lambda x: Path(x).stem[:20])\n",
    "        fig, ax = plt.subplots(figsize=(max(7, len(_pf_summary) * 1.5), 4))\n",
    "        ax.bar(x - w / 2, _pf_summary['Hybrids'],  w, label='Hybrids',  color='tomato')\n",
    "        ax.bar(x + w / 2, _pf_summary['Clusters'], w, label='Clusters', color='mediumpurple')\n",
    "        ax.set_xticks(x); ax.set_xticklabels(labels, rotation=30, ha='right')\n",
    "        ax.set_ylabel('Count'); ax.set_title('Hybrid & Cluster Motifs Across Files')\n",
    "        ax.legend(); plt.tight_layout()\n",
    "        _savefig(fig, _master_dir / 'comparative_hybrid_cluster.png')\n",
    "\n",
    "    # 10. GFF feature x class heatmap (if GFF data available)\n",
    "    if not _gff_df.empty:\n",
    "        _piv = _gff_df.groupby(['GFF_Type', 'Class']).size().unstack(fill_value=0)\n",
    "        fig, ax = plt.subplots(figsize=(max(10, len(_piv.columns) * 1.2),\n",
    "                                        max(4,  len(_piv) * 0.6)))\n",
    "        sns.heatmap(_piv, annot=True, fmt='d', cmap='YlOrRd', ax=ax,\n",
    "                    linewidths=0.4, cbar_kws={'label': 'Motif count'})\n",
    "        ax.set_title('GFF Feature Type x Non-B Class Heatmap')\n",
    "        ax.set_xlabel('Non-B Class'); ax.set_ylabel('GFF Feature Type')\n",
    "        plt.tight_layout()\n",
    "        _savefig(fig, _master_dir / 'comparative_gff_class_heatmap.png')\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY TABLES (displayed inline)\n",
    "# =============================================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('SUMMARY TABLES')\n",
    "print('=' * 70)\n",
    "\n",
    "for tname, tdf in _tables.items():\n",
    "    print(f'\\n--- {tname.replace(\"_\", \" \").title()} ---')\n",
    "    display(tdf)\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL COMPREHENSIVE GENOME STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "if not _master_df.empty:\n",
    "    _all_motifs_global = _master_df.to_dict('records')\n",
    "    _total_bp_global   = max(\n",
    "        sum(sum(r['seq_lengths'].values()) for r in RESULTS_BY_FILE.values()), 1)\n",
    "    try:\n",
    "        gstats = compute_comprehensive_genome_stats(_all_motifs_global, _total_bp_global)\n",
    "        _gsg_rows = [\n",
    "            ('Genome Length',                    f\"{gstats['genome_length']:,} bp\"),\n",
    "            ('Motifs (excl. Hybrid/Cluster)',     f\"{gstats['n_motifs']:,}\"),\n",
    "            ('Motifs (incl. Hybrid/Cluster)',     f\"{gstats['n_motifs_all']:,}\"),\n",
    "            ('Motif Classes',                     str(gstats['n_classes'])),\n",
    "            ('Motif Density',                     f\"{gstats['density_per_kb']:.4f} / kb\"),\n",
    "            ('Total Covered Bases',               f\"{gstats['total_covered_bases']:,} bp\"),\n",
    "            ('Coverage Fraction',                 f\"{gstats['coverage_fraction']:.6f}\"),\n",
    "            ('Coverage (%)',                      f\"{gstats['coverage_pct']:.4f}%\"),\n",
    "            ('Raw Occupancy',                     f\"{gstats['raw_occupancy_bp']:,} bp\"),\n",
    "            ('Normalized Occupancy (SLI)',        f\"{gstats['normalized_occupancy']:.6f}\"),\n",
    "            ('Mean Overlap Depth',                f\"{gstats['mean_overlap_depth']:.4f}\"),\n",
    "            ('SLI',                               f\"{gstats['sli']:.6f}\"),\n",
    "            ('Structural Intensity',              f\"{gstats['structural_intensity']:.6f}\"),\n",
    "            ('Weighted Structural Coverage',      f\"{gstats['weighted_structural_coverage']:.6f}\"),\n",
    "            ('Mean Inter-Motif Distance',         f\"{gstats['mean_inter_motif_distance']:.2f} bp\"),\n",
    "            ('CV (Clustering Coefficient)',       f\"{gstats['cv_spatial_clustering']:.4f}\"),\n",
    "            (f\"Max Local Density (W={gstats['window_size']:,} bp)\",\n",
    "                                                  f\"{gstats['max_local_density']:.6f}\"),\n",
    "            ('Max Class Diversity',               str(gstats['max_class_diversity_window'])),\n",
    "            ('Max Cluster Score',                 f\"{gstats['max_cluster_score']:.6f}\"),\n",
    "            ('Hybrid Regions',                    f\"{gstats['hybrid_count']:,}\"),\n",
    "            ('Hybrid Coverage',                   f\"{gstats['hybrid_coverage_pct']:.4f}%\"),\n",
    "            ('Cluster Regions',                   f\"{gstats['cluster_count']:,}\"),\n",
    "            ('Cluster Coverage',                  f\"{gstats['cluster_coverage_pct']:.4f}%\"),\n",
    "            ('Mean Overlap Fraction',             f\"{gstats['mean_overlap_fraction']:.4f}\"),\n",
    "            ('Simpson Diversity Index (D)',       f\"{gstats['simpson_diversity_index']:.4f}\"),\n",
    "            ('Effective Class Number (Neff)',     f\"{gstats['effective_class_number']:.4f}\"),\n",
    "            ('SCI (Structural Complexity Index)', f\"{gstats['sci']:.4f}\"),\n",
    "            ('Structural Dominance Ratio',        f\"{gstats['dominance_ratio']:.4f}\"),\n",
    "        ]\n",
    "        _gsg_df = pd.DataFrame(_gsg_rows, columns=['Metric', 'Value'])\n",
    "        print('\\n' + '=' * 70)\n",
    "        print('GLOBAL COMPREHENSIVE GENOME STATISTICS')\n",
    "        print('=' * 70)\n",
    "        display(_gsg_df)\n",
    "        _gsg_df.to_csv(str(_master_dir / 'global_comprehensive_genome_stats.csv'),\n",
    "                       encoding='utf-8-sig', index=False)\n",
    "    except Exception as _gse:\n",
    "        print(f'  \\u26a0\\ufe0f  Global comprehensive stats failed: {_gse}')\n",
    "\n",
    "# =============================================================================\n",
    "# DOWNLOAD LINKS\n",
    "# =============================================================================\n",
    "\n",
    "import base64\n",
    "\n",
    "_MIME = {\n",
    "    'csv':     'text/csv',\n",
    "    'xlsx':    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "    'parquet': 'application/octet-stream',\n",
    "    'png':     'image/png',\n",
    "}\n",
    "\n",
    "def _dl(path, label):\n",
    "    with open(path, 'rb') as fh:\n",
    "        b64 = base64.b64encode(fh.read()).decode()\n",
    "    ext  = Path(path).suffix.lstrip('.')\n",
    "    mime = _MIME.get(ext, 'application/octet-stream')\n",
    "    return (f'<a href=\"data:{mime};base64,{b64}\" download=\"{Path(path).name}\" '\n",
    "            f'style=\"margin:2px 6px;padding:3px 8px;border:1px solid #aaa;'\n",
    "            f'border-radius:4px;text-decoration:none;\">{label}</a>')\n",
    "\n",
    "_html = ['<h2>\\U0001f4e5 Downloads</h2><h3>Master Outputs</h3><div>']\n",
    "for fmt, fn in [('CSV', 'master_motifs.csv'), ('Parquet', 'master_motifs.parquet')]:\n",
    "    p = _master_dir / fn\n",
    "    if p.exists(): _html.append(_dl(str(p), f'Master {fmt}'))\n",
    "if (_master_dir / 'global_comprehensive_genome_stats.csv').exists():\n",
    "    _html.append(_dl(str(_master_dir / 'global_comprehensive_genome_stats.csv'),\n",
    "                     'Global Comprehensive Stats'))\n",
    "_html.append('</div><h3>Statistics Tables</h3><div>')\n",
    "for tn in _tables:\n",
    "    p = _master_dir / f'{tn}.csv'\n",
    "    if p.exists(): _html.append(_dl(str(p), tn.replace('_', ' ').title()))\n",
    "_html.append('</div><h3>Comparative Charts (PNG)</h3><div>')\n",
    "for fn in sorted(_master_dir.glob('comparative_*.png')):\n",
    "    _html.append(_dl(str(fn), fn.stem.replace('_', ' ').title()))\n",
    "_html.append('</div>')\n",
    "display(HTML('\\n'.join(_html)))\n",
    "\n",
    "_wall_total = time.perf_counter() - _WALL_START\n",
    "print(f'\\n\\u2705 All outputs saved to: {_BASE}')\n",
    "print(f'   Total wall time: {_wall_total:.1f}s | Peak RAM: {_mem_mb():.0f} MB')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}